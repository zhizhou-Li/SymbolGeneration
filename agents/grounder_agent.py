# -*- coding: utf-8 -*-
# æ–‡ä»¶è·¯å¾„: SymbolGeneration/Agent/agents/grounder_agent.py
from __future__ import annotations
import json, re, requests
from typing import Dict, Any, Optional, List, Tuple
from bs4 import BeautifulSoup
from openai import OpenAI

from ..utils import log, save_json, extract_json
from ..config import OPENAI_API_KEY, MODELS

client = OpenAI(api_key=OPENAI_API_KEY)

# --- Endpoints ---
WIKI_SEARCH = "https://{lang}.wikipedia.org/w/api.php"
WIKI_SUMMARY = "https://{lang}.wikipedia.org/api/rest_v1/page/summary/{title}"


# [å…³é”®å‡½æ•°] ç™¾åº¦å›¾ç‰‡æœç´¢ (JSON API ç‰ˆ)
def _search_baidu_image(keyword: str) -> Optional[str]:
    """
    ä½¿ç”¨ç™¾åº¦å›¾ç‰‡æœç´¢çš„åå° JSON æ¥å£ (acjson)ã€‚
    æ— éœ€ç¿»å¢™ï¼Œè§£æç¨³å®šï¼Œç›´æ¥è¿”å›å›¾ç‰‡ URLã€‚
    """
    print(f"ğŸ” [Baidu] æ­£åœ¨æœç´¢å›¾ç‰‡: {keyword}")
    try:
        url = "https://image.baidu.com/search/acjson"

        # ä¼ªè£…æˆæµè§ˆå™¨çš„æ»šåŠ¨åŠ è½½è¯·æ±‚
        params = {
            "tn": "resultjson_com",
            "logid": "8305096434442765369",
            "ipn": "rj",
            "ct": "201326592",
            "is": "",
            "fp": "result",
            "queryWord": keyword,
            "cl": "2",
            "lm": "-1",
            "ie": "utf-8",
            "oe": "utf-8",
            "adpicid": "",
            "st": "-1",
            "z": "",
            "ic": "0",
            "hd": "",
            "latest": "",
            "copyright": "",
            "word": keyword,
            "s": "",
            "se": "",
            "tab": "",
            "width": "",
            "height": "",
            "face": "0",
            "istype": "2",
            "qc": "",
            "nc": "1",
            "fr": "",
            "expermode": "",
            "force": "",
            "pn": "0",
            "rn": "30",
            "gsm": "1e",
        }

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/plain, */*; q=0.01",
            "Referer": "https://image.baidu.com/search/index",
            "X-Requested-With": "XMLHttpRequest",
        }

        res = requests.get(url, params=params, headers=headers, timeout=8)

        if res.status_code == 200:
            try:
                # å¤„ç†éæ ‡å‡† JSON çš„è½¬ä¹‰å­—ç¬¦
                json_str = res.text.replace(r"\'", "'")
                data = json.loads(json_str)

                if "data" not in data or not isinstance(data["data"], list):
                    return None

                candidates = []

                # 1. æ”¶é›†å€™é€‰å›¾ (éå†æ‰€æœ‰è¿”å›çš„ 30 å¼ å›¾)
                for item in data["data"]:
                    if not isinstance(item, dict): continue

                    # ä¼˜å…ˆå– thumbURL (ç¼©ç•¥å›¾ï¼Œé“¾æ¥ç¨³å®š)
                    img_url = item.get("thumbURL") or item.get("middleURL")
                    if not img_url: continue

                    # è·å–å°ºå¯¸ä¿¡æ¯
                    w = int(item.get("width", 0) or 0)
                    h = int(item.get("height", 0) or 0)

                    if w > 200 and h > 200:
                        print(f"âœ… [Baidu] é€‰ä¸­é¦–å¼ æ¸…æ™°å›¾ç‰‡: {img_url[:50]}...")
                        return img_url

                # 2. [æ™ºèƒ½ç­›é€‰] ä¼˜å…ˆæ‰¾æ¨ªæ„å›¾ (é•¿å®½æ¯” > 1.2)
                # è¿™ç§å›¾ç‰‡é€šå¸¸æ˜¯åœ°æ ‡çš„å…¨æ™¯ç…§ï¼Œèƒ½è®© Detector è¯†åˆ«å‡º"èººç€"
                best_match = None
                for cand in candidates:
                    # è¿‡æ»¤å¤ªå°çš„å›¾
                    if cand["w"] < 200 or cand["h"] < 150: continue

                    # å…³é”®æ¡ä»¶ï¼šå¿…é¡»æ˜¯æ¨ªå‘çš„
                    if cand["ratio"] > 1.2:
                        best_match = cand["url"]
                        print(f"âœ… [Smart Pick] é€‰ä¸­æ¨ªå‘å…¨æ™¯å›¾ (W:{cand['w']} H:{cand['h']}): {best_match[:50]}...")
                        break

                # 3. å…œåº•ï¼šå¦‚æœå…¨æ˜¯ç«–å›¾ï¼Œæ²¡åŠæ³•ï¼Œåªèƒ½ç”¨ç¬¬ä¸€å¼ 
                if not best_match and candidates:
                    best_match = candidates[0]["url"]
                    print(f"âš ï¸ [Fallback] æœªæ‰¾åˆ°å®Œç¾æ„å›¾ï¼Œä½¿ç”¨é¦–å¼ ç»“æœ: {best_match[:50]}...")

                return best_match

            except Exception as e:
                print(f"âš ï¸ ç™¾åº¦è¿”å›æ•°æ®è§£æå¤±è´¥: {e}")
                pass

    except Exception as e:
        print(f"âš ï¸ ç™¾åº¦æœå›¾å¤±è´¥: {e}")

    return None


def _gather_raw_knowledge(user_text: str, search_focus: str = None) -> Tuple[str, Optional[str]]:
    queries = _expand_queries(user_text)
    blobs = []
    first_image = None

    # å¦‚æœæœ‰ç²¾å‡†æœç´¢è¯ï¼ŒæŠŠå®ƒåŠ åˆ°æŸ¥è¯¢åˆ—è¡¨çš„æœ€å‰é¢ï¼
    if search_focus:
        queries.insert(0, search_focus)

    has_chinese = any('\u4e00' <= ch <= '\u9fff' for ch in user_text)

    for q in queries:
        # 1. å°è¯•ç™¾åº¦ç™¾ç§‘
        if has_chinese:
            summary, img = _fetch_baidu_baike(q)
            if summary:
                blobs.append(f"[Baidu] {q}\n{summary}")
                if not first_image and img: first_image = img

                # å¦‚æœç™¾ç§‘æ²¡å›¾ï¼Œç”¨å½“å‰çš„ query (q) å»æœå›¾
                if not first_image:
                    first_image = _search_baidu_image(q)
                continue

                # 2. ç»´åŸºç™¾ç§‘é€»è¾‘ (ä¿æŒä¸å˜)
        # ... (ç•¥ï¼Œä¿æŒåŸä»£ç ) ...

    # 3. [æ ¸å¿ƒä¿®æ”¹] æœ€ç»ˆå…œåº•ï¼šä¼˜å…ˆä½¿ç”¨ç²¾å‡†è¯æœå›¾ï¼Œè€Œä¸æ˜¯ç”¨é•¿å¥å­
    if not first_image and has_chinese:
        # å¦‚æœæœ‰ search_focus (å¦‚"å…°å·ç™½å¡”å±±")ï¼Œç”¨å®ƒæœï¼
        target_keyword = search_focus if search_focus else user_text
        print(f"ğŸ” æœ€ç»ˆå…œåº•ï¼šå°è¯•ä½¿ç”¨å…³é”®è¯æœç´¢å›¾ç‰‡: {target_keyword}")
        first_image = _search_baidu_image(target_keyword)

    text = "\n\n".join(blobs)
    log("Grounder_raw", text if text else "(empty)")

    return text, first_image


# [ä¿®æ”¹] æ¥å£å¢åŠ  search_focus
def ground_entity_to_spec(user_text: str, search_focus: str = None) -> Dict[str, Any]:
    # ä¼ é€’ search_focus ç»™æœå›¾é€»è¾‘
    raw_text, ref_image_url = _gather_raw_knowledge(user_text, search_focus=search_focus)

    if not raw_text and not ref_image_url:
        spec = {"entity": {"name": user_text}, "constraints": {"must_not": []}}
        save_json("Grounder_spec", spec)
        return spec

    # ... (ä¸­é—´ LLM è°ƒç”¨ä»£ç ä¿æŒä¸å˜) ...
    msg_user = [
        {"type": "text", "text": f"User intent:\n{user_text}"},
        {"type": "text", "text": f"Raw encyclopedia snippets:\n{raw_text}"}
    ]
    resp = client.chat.completions.create(
        model=MODELS["LLM_MODEL"],
        response_format={"type": "json_object"},
        messages=[{"role": "system", "content": SYSTEM_TO_SPEC}, {"role": "user", "content": msg_user}]
    )
    spec = extract_json(resp.choices[0].message.content) or {"entity": {"name": user_text}}

    if not spec.get("constraints"): spec["constraints"] = {}
    spec["constraints"].setdefault("must_not", [])

    if ref_image_url:
        spec["reference_image_url"] = ref_image_url

    save_json("Grounder_spec", spec)
    return spec


# ----------------- Baidu Baike Helper -----------------
def _fetch_baidu_baike(keyword: str) -> Tuple[Optional[str], Optional[str]]:
    url = f"https://baike.baidu.com/item/{keyword}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        resp = requests.get(url, headers=headers, timeout=5, allow_redirects=True)
        if resp.status_code != 200:
            return None, None

        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')

        # 1. æå–æ–‡æœ¬
        texts = []
        summary_div = soup.find('div', class_='lemma-summary')
        if summary_div:
            texts.append(summary_div.get_text().strip())

        basic_info = soup.find('div', class_='basic-info')
        if basic_info:
            names = basic_info.find_all('dt')
            values = basic_info.find_all('dd')
            for n, v in zip(names, values):
                texts.append(f"{n.get_text().strip()}: {v.get_text().strip()}")

        summary_text = "\n".join(texts)
        if not summary_text: return None, None

        # 2. å°è¯•ä»ç™¾ç§‘æå–å›¾ç‰‡ (ä»…ä½œä¸ºå°è¯•)
        image_url = None
        meta_img = soup.find('meta', property="og:image")
        if meta_img:
            image_url = meta_img.get("content")

        if not image_url:
            pic_div = soup.find('div', class_='summary-pic')
            if pic_div:
                img = pic_div.find('img')
                if img: image_url = img.get('src')

        if image_url:
            if image_url.startswith('//'):
                image_url = "https:" + image_url
            elif image_url.startswith('/'):
                image_url = "https://baike.baidu.com" + image_url

        return summary_text, image_url

    except Exception as e:
        print(f"âš ï¸ Baidu Baike fetch error: {e}")
        return None, None


# ----------------- Small Helpers (Wiki) -----------------
def _wiki_search(q: str, lang="en") -> Optional[str]:
    try:
        params = {"action": "opensearch", "search": q, "limit": 1, "namespace": 0, "format": "json"}
        r = requests.get(WIKI_SEARCH.format(lang=lang), params=params, timeout=5)
        if r.status_code == 200:
            j = r.json()
            if isinstance(j, list) and len(j) >= 2 and j[1]: return j[1][0]
    except Exception:
        pass
    return None


def _wiki_summary(title: str, lang="en") -> Optional[Dict[str, Any]]:
    try:
        url = WIKI_SUMMARY.format(lang=lang, title=title.replace(" ", "_"))
        r = requests.get(url, timeout=5, headers={"accept": "application/json"})
        if r.status_code == 200:
            return r.json()
    except Exception:
        pass
    return None


def _expand_queries(user_text: str) -> List[str]:
    qs: List[str] = [user_text.strip()]
    for seg in re.findall(r"[ä¸€-é¾¥A-Za-z0-9Â·\-\s]{2,}", user_text):
        s = seg.strip()
        if s and s not in qs: qs.append(s)
    return list(dict.fromkeys(qs))


def _langs_for(q: str, user_text: str) -> List[str]:
    has_chinese = any('\u4e00' <= ch <= '\u9fff' for ch in user_text + q)
    return ["zh", "en"] if has_chinese else ["en", "zh"]


# ----------------- Main Logic -----------------
def _gather_raw_knowledge(user_text: str) -> Tuple[str, Optional[str]]:
    queries = _expand_queries(user_text)
    blobs = []
    first_image = None

    has_chinese = any('\u4e00' <= ch <= '\u9fff' for ch in user_text)

    for q in queries:
        # 1. å°è¯•ç™¾åº¦ç™¾ç§‘
        if has_chinese:
            summary, img = _fetch_baidu_baike(q)
            if summary:
                blobs.append(f"[Baidu] {q}\n{summary}")

                # å¦‚æœç™¾ç§‘æœ‰å›¾ï¼Œæš‚å­˜
                if not first_image and img:
                    first_image = img

                # [å…³é”®] å¦‚æœç™¾ç§‘æœ‰æ–‡ä½†æ²¡å›¾ï¼Œè°ƒç”¨ç™¾åº¦å›¾ç‰‡æœç´¢è¡¥æ•‘
                if not first_image:
                    first_image = _search_baidu_image(q)

                continue

                # 2. å°è¯•ç»´åŸºç™¾ç§‘
        langs = _langs_for(q, user_text)
        for lang in langs:
            title = _wiki_search(q, lang)
            if title:
                data = _wiki_summary(title, lang)
                if data:
                    extract = data.get("extract")
                    img_src = data.get("thumbnail", {}).get("source") or data.get("originalimage", {}).get("source")

                    if extract:
                        blobs.append(f"[Wiki-{lang}] {title}\n{extract}")
                        if not first_image and img_src:
                            first_image = img_src
                        break

    # 3. [æœ€åå…œåº•] ä»ç„¶æ²¡å›¾ï¼Ÿç”¨åŸè¯å»ç™¾åº¦å›¾ç‰‡æœä¸€æŠŠ
    if not first_image and has_chinese:
        print(f"ğŸ” æœ€ç»ˆå…œåº•ï¼šå°è¯•ä½¿ç”¨ç™¾åº¦æœç´¢å›¾ç‰‡: {user_text}")
        first_image = _search_baidu_image(user_text)

    text = "\n\n".join(blobs)
    log("Grounder_raw", text if text else "(empty)")

    if first_image:
        log("Grounder_image", first_image)

    return text, first_image


# (SYSTEM_TO_SPEC ä¿æŒä¸å˜)
SYSTEM_TO_SPEC = (
    "You are a visual knowledge extraction expert. "
    "Your task is to convert vague user intent and raw encyclopedia snippets into a STRICT visual structure spec.\n"
    "Goal: Extract specific physical constraints so a blind painter can reconstruct the landmark accurately.\n"
    "Schema:\n"
    "{ \n"
    "  \"entity\": {\"name\": str, \"location\": str},\n"
    "  \"entity_type\": \"bridge|tower|building|statue|logogram|other\",\n"
    "  \"structure\": {\n"
    "      \"structural_system\": \"truss|arch|suspension|beam|unknown\",\n"
    "      \"shape_features\": [str],  // e.g. \"3 spans\", \"octagonal base\", \"reclining posture\"\n"
    "      \"material\": \"steel|stone|concrete|wood\",\n"
    "      \"view_recommendation\": \"side|front|isometric\"\n"
    "  },\n"
    "  \"constraints\": {\n"
    "      \"must\": [str],      // Visual elements that MUST appear\n"
    "      \"must_not\": [str]   // Elements to EXCLUDE\n"
    "  }\n"
    "}\n"
    "Rules:\n"
    "1. Rely HEAVILY on the provided snippets.\n"
    "2. If snippets describe a statue, extract posture and composition details.\n"
    "3. Return ONLY a JSON object."
)


def ground_entity_to_spec(user_text: str) -> Dict[str, Any]:
    raw_text, ref_image_url = _gather_raw_knowledge(user_text)

    if not raw_text and not ref_image_url:
        spec = {"entity": {"name": user_text}, "constraints": {"must_not": []}}
        save_json("Grounder_spec", spec)
        return spec

    msg_user = [
        {"type": "text", "text": f"User intent:\n{user_text}"},
        {"type": "text", "text": f"Raw encyclopedia snippets:\n{raw_text}"}
    ]
    resp = client.chat.completions.create(
        model=MODELS["LLM_MODEL"],
        response_format={"type": "json_object"},
        messages=[{"role": "system", "content": SYSTEM_TO_SPEC}, {"role": "user", "content": msg_user}]
    )
    spec = extract_json(resp.choices[0].message.content) or {"entity": {"name": user_text}}

    if not spec.get("constraints"): spec["constraints"] = {}
    spec["constraints"].setdefault("must_not", [])

    if ref_image_url:
        spec["reference_image_url"] = ref_image_url

    save_json("Grounder_spec", spec)
    return spec